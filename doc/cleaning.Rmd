---
title: "Project 4 data cleaning"
output: html_notebook
---
```{r}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(text2vec, dplyr, qlcMatrix, kernlab, knitr)
library(stringr)
```

```{r}
data.lib="../data/nameset"
data.files=list.files(path=data.lib, "*.txt")

data.files

## remove "*.txt"
query.list=substring(data.files, 
                     1, nchar(data.files)-4)

query.list

## add a space
# query.list=paste(substring(query.list, 1, 1), 
#                  " ", 
#                  substring(query.list, 
#                            2, nchar(query.list)),
#                  sep=""
#                  )

query.list

```

```{r}
f.line.proc=function(lin, nam.query="."){

  # remove unwanted characters
  char_notallowed <- "\\@#$%^&?" # characters to be removed
  lin.str=str_replace(lin, char_notallowed, "")

  # get author id
  lin.str=strsplit(lin.str, "_")[[1]]
  author_id=as.numeric(lin.str[1])
  
  # get paper id
  lin.str=lin.str[2]
  paper_id=strsplit(lin.str, " ")[[1]][1]
  lin.str=substring(lin.str, nchar(paper_id)+1, nchar(lin.str))
  paper_id=as.numeric(paper_id)
  
  # get coauthor list
  lin.str=strsplit(lin.str, "<>")[[1]]
  coauthor_list=strsplit(lin.str[1], ";")[[1]]

  #print(lin.str)
  for(j in 1:length(coauthor_list)){
      if(nchar(coauthor_list[j])>0){
        nam = strsplit(coauthor_list[j], " ")[[1]]
        if(nchar(nam[1])>0){
          first.ini=substring(nam[1], 1, 1)
        }else{
          first.ini=substring(nam[2], 1, 1)
        }
      }
      last.name=nam[length(nam)]
      nam.str = paste0(first.ini, last.name)
      coauthor_list[j]=nam.str
  }
  
  match_ind = charmatch(nam.query, coauthor_list, nomatch=-1)
  
  #print(nam.query)
  #print(coauthor_list)
  #print(match_ind)
  
  if(match_ind>0){
    coauthor_list=coauthor_list[-match_ind]
  }
  
  coauthor_list = paste(coauthor_list, collapse = " ")

  
  paper_title=lin.str[2]
  journal_name=lin.str[3] 
  
  list(author_id = author_id,
       unique_paper_id = paste(nam.query, author_id, paper_id, sep = "_"),
       unique_author_id = paste(nam.query, author_id, sep = "_"),
       paper_id = paper_id, 
       coauthor_list = coauthor_list, 
       paper_title = paper_title, 
       journal_name = journal_name,
       combined = paste(coauthor_list, paper_title, journal_name)
       )
}
```

```{r}
## generate DTM/tfidf for 14 files individually

data_list = list(1:length(data.files))

for(i in 1:length(data.files)){
  
  ## Step 0 scan in one line at a time.
  
  dat=as.list(readLines(paste(data.lib, data.files[i], sep="/")))
  data_list[[i]]=lapply(dat, f.line.proc, nam.query=query.list[i])
}

# tfidf_list <- list()
dtm_list <- list()
for (file in 1:length(data_list)) {
  a2 <- list()
  for (i in 1:length(data_list[[file]])) {
    a2 <- rbind(a2, data_list[[file]][[i]])
  }
  
  a2 <- data.frame(a2)
  a2 <- sapply(a2, unlist)
  a2 <- data.frame(a2)
  a2$combined <- as.character(a2$combined)
  
  it_train2 <- #itoken(paste(a2$paper_title, a2$coauthor_list, a2$journal_name), 
               itoken(a2$combined, 
               preprocessor = tolower, 
               tokenizer = word_tokenizer,
               ids = a2$unique_paper_id,
               # turn off progressbar because it won't look nice in rmd
               progressbar = FALSE)
  vocab2 <- create_vocabulary(it_train2, stopwords = c("a", "an", "the", "in", "on",
                                                     "at", "of", "above", "under"))
  
  vectorizer <- vocab_vectorizer(vocab2)
  dtm_train <- create_dtm(it_train2, vectorizer)
  
  dtm_i <- list(name = query.list[file], dtm = dtm_train)
  dtm_list[[file]]  <- dtm_i
  
  # tfidf <- TfIdf$new()
  # dtm_train_tfidf <- fit_transform(dtm_train, tfidf)
  # tfidf_i <- list(name = query.list[file], tfidf = dtm_train_tfidf)
  # tfidf_list[[file]] <- tfidf_i
}

save(dtm_list, file = "../output/dtm_list.RData")
# save(tfidf_list, file = "../output/tfidf_list.RData")
```

```{r}
## generate DTM/tfidf for all 14 files combined as one

data_list = list(1)
dat = list()

for(i in 1:length(data.files)){
  
  ## Step 0 scan in one line at a time.
  
  raw_dat=as.list(readLines(paste(data.lib, data.files[i], sep="/")))
  dat <- c(dat, raw_dat)
  
}

data_list=lapply(dat, f.line.proc, nam.query=query.list[i])

# tfidf_list_combined <- list()
dtm_list_combined <- list()
a2 <- list()
for (line in 1:length(data_list)) {
  a2 <- rbind(a2, data_list[[line]])
  }
  
a2 <- data.frame(a2)
a2 <- sapply(a2, unlist)
a2 <- data.frame(a2)
a2$combined <- as.character(a2$combined)

it_train2 <- itoken(a2$combined, 
                    preprocessor = tolower, 
                    tokenizer = word_tokenizer,
                    ids = a2$unique_paper_id,
                    # turn off progressbar because it won't look nice in rmd
                    progressbar = FALSE)
vocab2 <- create_vocabulary(it_train2, stopwords = c("a", "an", "the", "in", "on",
                                                     "at", "of", "above", "under"))

vectorizer <- vocab_vectorizer(vocab2)
dtm_list_combined <- create_dtm(it_train2, vectorizer)

# tfidf <- TfIdf$new()
# tfidf_list_combined <- fit_transform(dtm_list_combined, tfidf)


save(dtm_list_combined, file = "../output/dtm_list_combined.RData")
# save(tfidf_list_combined, file = "../output/tfidf_list_combined.RData")
```

